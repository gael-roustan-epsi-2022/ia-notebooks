{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Linear Regression with Synthetic Data.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gael-roustan-epsi-2022/ia-notebooks/blob/main/Linear_Regression_with_Synthetic_Data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wDlWLbfkJtvu"
      },
      "source": [
        "#@title Copyright 2020 Google LLC. Double-click here for license information.\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TL5y5fY9Jy_x"
      },
      "source": [
        "# Simple Linear Regression with Synthetic Data\n",
        "\n",
        "Nous allons expérimenter colab et tf.keras avec une base de données prête à l'emploi."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bMr7MPVmoiHf"
      },
      "source": [
        "## Utiliser la bonne version de TensorFlow\n",
        "\n",
        "Le code suivant permet de fixer la version de tensorflow que l'on souhaite utiliser, à savoir la plus récente, la 2.x (2.8)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z1pOWL7eevO8"
      },
      "source": [
        "#@title Run this Colab on TensorFlow 2.x\n",
        "%tensorflow_version 2.x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xchnxAsaKKqO"
      },
      "source": [
        "## Import des modules essentiels à la suite des opérations\n",
        "\n",
        "La cellule suivante importe les 3 modules que nous souhaitons utiliser :\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9n9_cTveKmse"
      },
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from matplotlib import pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SIpsyJITPcbG"
      },
      "source": [
        "## Fonctions pour construire et entraîner le modèle\n",
        "\n",
        "Les 2 fonctions ci-après :\n",
        "\n",
        "  * `build_model(my_learning_rate)`, construit un modèle vide.\n",
        "  * `train_model(model, feature, label, epochs)`, entraîne un modèle à partir des données exemples (caractéristique et résultat) qu'on lui passe. \n",
        "\n",
        "Il n'y a aucun code à ajouter/modifier de votre part."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xvO_beKVP1Ke"
      },
      "source": [
        "#@title Fonctions de création et d'entrainement de modèle\n",
        "def build_model(my_learning_rate):\n",
        "  \"\"\"Create and compile a simple linear regression model.\"\"\"\n",
        "  # Most simple tf.keras models are sequential. \n",
        "  # A sequential model contains one or more layers.\n",
        "  model = tf.keras.models.Sequential()\n",
        "\n",
        "  # Describe the topography of the model.\n",
        "  # The topography of a simple linear regression model\n",
        "  # is a single node in a single layer. \n",
        "  model.add(tf.keras.layers.Dense(units=1, \n",
        "                                  input_shape=(1,)))\n",
        "\n",
        "  # Compile the model topography into code that \n",
        "  # TensorFlow can efficiently execute. Configure \n",
        "  # training to minimize the model's mean squared error. \n",
        "  model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=my_learning_rate),\n",
        "                loss=\"mean_squared_error\",\n",
        "                metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
        "\n",
        "  return model           \n",
        "\n",
        "\n",
        "def train_model(model, feature, label, epochs, batch_size):\n",
        "  \"\"\"Train the model by feeding it data.\"\"\"\n",
        "\n",
        "  # Feed the feature values and the label values to the \n",
        "  # model. The model will train for the specified number \n",
        "  # of epochs, gradually learning how the feature values\n",
        "  # relate to the label values. \n",
        "  history = model.fit(x=feature,\n",
        "                      y=label,\n",
        "                      batch_size=batch_size,\n",
        "                      epochs=epochs)\n",
        "\n",
        "  # Gather the trained model's weight and bias.\n",
        "  trained_weight = model.get_weights()[0]\n",
        "  trained_bias = model.get_weights()[1]\n",
        "\n",
        "  # The list of epochs is stored separately from the \n",
        "  # rest of history.\n",
        "  epochs = history.epoch\n",
        "  \n",
        "  # Gather the history (a snapshot) of each epoch.\n",
        "  hist = pd.DataFrame(history.history)\n",
        "\n",
        "  # Specifically gather the model's root mean \n",
        "  #squared error at each epoch. \n",
        "  rmse = hist[\"root_mean_squared_error\"]\n",
        "\n",
        "  return trained_weight, trained_bias, epochs, rmse\n",
        "\n",
        "print(\"Defined create_model and train_model\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ak_TMAzGOIFq"
      },
      "source": [
        "## Fonctions de visualisation\n",
        "\n",
        "C'est  [Matplotlib](https://developers.google.com/machine-learning/glossary/#matplotlib) qui est utilisé pour visualiser les 2 graphiques :\n",
        "\n",
        "* un graphique qui représente la caractéristique en fonction du résultat\n",
        "* une [courbe d'écart](https://developers.google.com/machine-learning/glossary/#loss_curve).\n",
        "\n",
        "Idem, pas de code à modifier, supprimer ou ajouter de votre part."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QF0BFRXTOeR3"
      },
      "source": [
        "#@title Define the plotting functions\n",
        "def plot_the_model(trained_weight, trained_bias, feature, label):\n",
        "  \"\"\"Plot the trained model against the training feature and label.\"\"\"\n",
        "\n",
        "  # Label the axes.\n",
        "  plt.xlabel(\"feature\")\n",
        "  plt.ylabel(\"label\")\n",
        "\n",
        "  # Plot the feature values vs. label values.\n",
        "  plt.scatter(feature, label)\n",
        "\n",
        "  # Create a red line representing the model. The red line starts\n",
        "  # at coordinates (x0, y0) and ends at coordinates (x1, y1).\n",
        "  x0 = 0\n",
        "  y0 = trained_bias\n",
        "  x1 = feature[-1]\n",
        "  y1 = trained_bias + (trained_weight * x1)\n",
        "  plt.plot([x0, x1], [y0, y1], c='r')\n",
        "\n",
        "  # Render the scatter plot and the red line.\n",
        "  plt.show()\n",
        "\n",
        "def plot_the_loss_curve(epochs, rmse):\n",
        "  \"\"\"Plot the loss curve, which shows loss vs. epoch.\"\"\"\n",
        "\n",
        "  plt.figure()\n",
        "  plt.xlabel(\"Epoch\")\n",
        "  plt.ylabel(\"Root Mean Squared Error\")\n",
        "\n",
        "  plt.plot(epochs, rmse, label=\"Loss\")\n",
        "  plt.legend()\n",
        "  plt.ylim([rmse.min()*0.97, rmse.max()])\n",
        "  plt.show()\n",
        "\n",
        "print(\"Defined the plot_the_model and plot_the_loss_curve functions.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LVSDPusELEZ5"
      },
      "source": [
        "## Définition du jeu de données\n",
        "\n",
        "Le jeu de données contient 12 [exemples](https://developers.google.com/machine-learning/glossary/#example). Chaque exemple est constitué d'une [caractéristique](https://developers.google.com/machine-learning/glossary/#feature) et d'un [résultat associé](https://developers.google.com/machine-learning/glossary/#label).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnUSYKw4LUuh"
      },
      "source": [
        "my_feature = ([1.0, 2.0,  3.0,  4.0,  5.0,  6.0,  7.0,  8.0,  9.0, 10.0, 11.0, 12.0])\n",
        "my_label   = ([5.0, 8.8,  9.6, 14.2, 18.8, 19.5, 21.4, 26.8, 28.9, 32.0, 33.8, 38.2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K24afla-4s2x"
      },
      "source": [
        "## Personnaliser les hyperparamètres\n",
        "\n",
        "Les hyperparamètres dans ce script sont les suivants:\n",
        "\n",
        "  * [taux d'apprentissage](https://developers.google.com/machine-learning/glossary/#learning_rate)\n",
        "  * [essais](https://developers.google.com/machine-learning/glossary/#epoch)\n",
        "  * [taille du jeu de données](https://developers.google.com/machine-learning/glossary/#batch_size)\n",
        "\n",
        "La prochaine section initialise les hyperparamètres."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ye730h13CQ97"
      },
      "source": [
        "learning_rate=0.01\n",
        "epochs=10\n",
        "my_batch_size=12\n",
        "\n",
        "my_model = build_model(learning_rate)\n",
        "trained_weight, trained_bias, epochs, rmse = train_model(my_model, my_feature, \n",
        "                                                         my_label, epochs,\n",
        "                                                         my_batch_size)\n",
        "plot_the_model(trained_weight, trained_bias, my_feature, my_label)\n",
        "plot_the_loss_curve(epochs, rmse)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QwSm60H6pQjJ"
      },
      "source": [
        "## Exercice 1: Examiner les graphiques\n",
        "\n",
        "Analyser en premier le graphique du haut. Les points bleus représentent les valeurs réellement obtenus et la ligne rouge représente la sortie du modèle entraîné. Idéalement, la ligne rouge passe par tous les points bleus.\n",
        "\n",
        "A ce stage, normalement, la ligne rouge est loin de passer par tous les points bleus.\n",
        "\n",
        "Examiner le graphique du bas, qui montre l'évolution de l'erreur. Remarquez que la courbe ne s'aplatit pas, ce qui signifie que l'on atteint pas le comportement limite, c'est donc que le modèle n'as pas été assez entraîne.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lLXPvqCRvgI4"
      },
      "source": [
        "## Exercice 2: Augmenter le nombre de tests à réaliser\n",
        "\n",
        "L'écart d'entraînement devrait décroitre, rapidement au début, puis plus lentement au fur et à mesure jusqu'à avoir un coefficient directeur proche de 0, ce qui signifierai qu'elle [converge](http://developers.google.com/machine-learning/glossary/#convergence).\n",
        "\n",
        "Dans l'exercice 1, l'écart d'entraînement ne converge pas. L'une des possibilités est qu'il entraîner le modèle sur un plus grand nombre d'itérations.\n",
        "Vous devez augmenter le nombre d'epochs afin de constater que l'écart d'entraînement converge. Augmenter la valeur par petits écarts (pas plus de 100).\n",
        "\n",
        "Est-ce que le modèle finit par converger ?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uXuJH3h6t5qs"
      },
      "source": [
        "learning_rate=0.01\n",
        "epochs= ?   # Replace ? with an integer.\n",
        "my_batch_size=12\n",
        "\n",
        "my_model = build_model(learning_rate)\n",
        "trained_weight, trained_bias, epochs, rmse = train_model(my_model, my_feature, \n",
        "                                                        my_label, epochs,\n",
        "                                                        my_batch_size)\n",
        "plot_the_model(trained_weight, trained_bias, my_feature, my_label)\n",
        "plot_the_loss_curve(epochs, rmse)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0KmzfFB5zwvd"
      },
      "source": [
        "## Exercice 3: Augmenter le taux d'apprentissage\n",
        "\n",
        "Dans l'exercice précédent, nous avons augmenté le nombre d'itérations pour faire converger le modèle. Il est également possible de faire converger le modèle plus rapidement, c'est à dire avec moins d'itérations, en augmenter le taux d'apprentissage. Attention cependant à ne pas augmenter trop le taux d'apprentissage, ce qui risque de rendre impossible la convergence de votre modèle.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eD1hTmdd0uCo"
      },
      "source": [
        "# Increase the learning rate and decrease the number of epochs.\n",
        "learning_rate=200 \n",
        "epochs=500 \n",
        "\n",
        "my_model = build_model(learning_rate)\n",
        "trained_weight, trained_bias, epochs, rmse = train_model(my_model, my_feature, \n",
        "                                                         my_label, epochs,\n",
        "                                                         my_batch_size)\n",
        "plot_the_model(trained_weight, trained_bias, my_feature, my_label)\n",
        "plot_the_loss_curve(epochs, rmse)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c96ITm021NEV"
      },
      "source": [
        "Le résultat n'est pas bon, puisque la ligne rouge ne s'aligne pas du tout avec les points bleus. De plus, la courbe de l'écart ressemble à des montagnes russes et ne converge donc jamais.\n",
        "Une courbe avec une allure oscillatoire de la sorte marque un taux d'apprentissage trop élevé."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r63YkMx82WVr"
      },
      "source": [
        "## Exercice 4: Trouver la combinaison idéale entre le nombre de tests et le taux d'apprentissage\n",
        "\n",
        "Essyare différentes valeurs pour les 2 hyperparamètres suivants afin que la courbe de l'écart converge le plus rapidement possible\n",
        "\n",
        "*  learning_rate\n",
        "*  epochs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZYC8eR5x5n4m"
      },
      "source": [
        "# Set the learning rate and number of epochs\n",
        "learning_rate= ?  # Replace ? with a floating-point number\n",
        "epochs= ?   # Replace ? with an integer\n",
        "\n",
        "my_model = build_model(learning_rate)\n",
        "trained_weight, trained_bias, epochs, rmse = train_model(my_model, my_feature, \n",
        "                                                         my_label, epochs,\n",
        "                                                         my_batch_size)\n",
        "plot_the_model(trained_weight, trained_bias, my_feature, my_label)\n",
        "plot_the_loss_curve(epochs, rmse)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0NDET9e6AAbA"
      },
      "source": [
        "## Exercice 5: Ajuster la taille du nombre d'échantillons considérés pour le calcul de l'écart\n",
        "\n",
        "Le système recalcule l'écart du modèle et ajuste le poids et le biais après chaque **itération**. Une itération est une portée dans laquelle le système traite un batch. Par exemple, si la **taille du batch** est 6, alors le système recalcul l'écart et ajuste le poids et le biais après avoir traité 6 examples.\n",
        "\n",
        "Un **epoch** porte suffissament d'itérations pour traiter chaque exemple dans le jeu de données. Par exemple, si la taille du batch est 12, alors chaque epoch dure 1 iteration. En revanche, si la taille du batch est 6, alors chaque epoch consomme 2 itérations.\n",
        "\n",
        "Il est toujours tentant de paramétrer la taille du batch au nombre d'exemples dans le jeu de données (12 dans notre cas). Cependant, le modèle peut être entrainé plus rapidement sur des batchs de plus petite taille. Et inversement, des batchs trop petits peuvent ne pas contenir suffisamment d'information à chaque fois pour aider le modèle à converger vers les valeurs de poids et de biais optimales.\n",
        "\n",
        "Essayer de voir les impacts de valeur du `batch_size` dans la cellule de code suivant. Quel est le plus petit entier pour lequel le modèle converge en moins d'une centaine d'epochs ?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_vGx0lOodQrT"
      },
      "source": [
        "learning_rate=0.05\n",
        "epochs=100\n",
        "my_batch_size= ?  # Replace ? with an integer.\n",
        "\n",
        "my_model = build_model(learning_rate)\n",
        "trained_weight, trained_bias, epochs, rmse = train_model(my_model, my_feature, \n",
        "                                                        my_label, epochs,\n",
        "                                                        my_batch_size)\n",
        "plot_the_model(trained_weight, trained_bias, my_feature, my_label)\n",
        "plot_the_loss_curve(epochs, rmse)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}