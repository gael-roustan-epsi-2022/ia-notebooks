{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Linear Regression with a Real Dataset.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gael-roustan-epsi-2022/ia-notebooks/blob/main/Linear_Regression_with_a_Real_Dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wDlWLbfkJtvu",
        "cellView": "form"
      },
      "source": [
        "#@title Copyright 2020 Google LLC. Double-click here for license information.\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TL5y5fY9Jy_x"
      },
      "source": [
        "# Linear Regression with a Real Dataset\n",
        "\n",
        "This Colab uses a real dataset to predict the prices of houses in California.   \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8wtceyJj2uX"
      },
      "source": [
        "## Objectifs de cet exercice :\n",
        "\n",
        "A la fin de cet exercice, vous serez en mesure de :\n",
        "  * lire un fichier .csv enregistré dans un objet [pandas](https://developers.google.com/machine-learning/glossary/#pandas) DataFrame.\n",
        "  * Analyser un [jeu de données](https://developers.google.com/machine-learning/glossary/#data_set). \n",
        "  * Expérimenter avec plusieurs [caractéristiques](https://developers.google.com/machine-learning/glossary/#feature) pour construire un modèle.\n",
        "  * Affiner le modèle et notamment ses [hyperparamètres](https://developers.google.com/machine-learning/glossary/#hyperparameter)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJZEgJQSjyK4"
      },
      "source": [
        "## Le jeu de données\n",
        "\n",
        "Cet exercice est à réaliser avec 2 jeux de données.\n",
        "\n",
        "Le 1er [jeu de données](https://developers.google.com/machine-learning/crash-course/california-housing-data-description) est basé sur les données immobilières de Californie.\n",
        "\n",
        "Le 2nd [jeu de données]() est basé sur les données immobilières de France.\n",
        "Attention, les étapes décrites pour nettoyer la donnée et le nom des colonnes ne correspond que pour le 1er jeu de données. A vous d'adapter pour le 2nd jeu de données."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tX_umRMMsa3z"
      },
      "source": [
        "## Utiliser la bonne version de tensorflow\n",
        "\n",
        "Le code caché s'assure que Colab utilise la bonne version de Tensorflow, à savoir 2.X\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lM75uNH-sTv2",
        "cellView": "form"
      },
      "source": [
        "#@title Run on TensorFlow 2.x\n",
        "%tensorflow_version 2.x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xchnxAsaKKqO"
      },
      "source": [
        "## Imporer les modules les plus importants\n",
        "\n",
        "Le code suivant importe les modules nécessaires au reste du code dans colab.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9n9_cTveKmse",
        "cellView": "form"
      },
      "source": [
        "#@title Import relevant modules\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# The following lines adjust the granularity of reporting. \n",
        "pd.options.display.max_rows = 10\n",
        "pd.options.display.float_format = \"{:.1f}\".format"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_TaJhU4KcuY"
      },
      "source": [
        "## Le jeu de données\n",
        "\n",
        "Les jeux de données sont souvent stockés sur disque ou disponible à une adresse internet au [format csv](https://wikipedia.org/wiki/Comma-separated_values). \n",
        "\n",
        "Un fichier csv correctement formaté contient les noms des colonnes dans la première ligne, suivi par plusieurs lignes de données. Une virgule sépare chaque valeur sur chaque ligne. Par exemple, ci-dessous les cinq premières lignes du fichier csv contenant les données immobilières de Californie.\n",
        "\n",
        "```\n",
        "\"longitude\",\"latitude\",\"housing_median_age\",\"total_rooms\",\"total_bedrooms\",\"population\",\"households\",\"median_income\",\"median_house_value\"\n",
        "-114.310000,34.190000,15.000000,5612.000000,1283.000000,1015.000000,472.000000,1.493600,66900.000000\n",
        "-114.470000,34.400000,19.000000,7650.000000,1901.000000,1129.000000,463.000000,1.820000,80100.000000\n",
        "-114.560000,33.690000,17.000000,720.000000,174.000000,333.000000,117.000000,1.650900,85700.000000\n",
        "-114.570000,33.640000,14.000000,1501.000000,337.000000,515.000000,226.000000,3.191700,73400.000000\n",
        "```\n",
        "\n",
        "Pour les valeurs foncières de France, nous avons plus de colonnes, donc sûrement plus d'étapes de préparation de données à venir\n",
        "```\n",
        "id_mutation,date_mutation,numero_disposition,nature_mutation,valeur_fonciere,adresse_numero,adresse_suffixe,adresse_nom_voie,adresse_code_voie,code_postal,code_commune,nom_commune,code_departement,ancien_code_commune,ancien_nom_commune,id_parcelle,ancien_id_parcelle,numero_volume,lot1_numero,lot1_surface_carrez,lot2_numero,lot2_surface_carrez,lot3_numero,lot3_surface_carrez,lot4_numero,lot4_surface_carrez,lot5_numero,lot5_surface_carrez,nombre_lots,code_type_local,type_local,surface_reelle_bati,nombre_pieces_principales,code_nature_culture,nature_culture,code_nature_culture_speciale,nature_culture_speciale,surface_terrain,longitude,latitude\n",
        "2021-1,2021-01-05,000001,Vente,185000,5080,,CHE DE VOGELAS,0471,01370,01426,Val-Revermont,01,,,01426312ZC0122,,,,,,,,,,,,,0,1,Maison,97,5,S,sols,,,2410,5.386094,46.32714\n",
        "2021-1,2021-01-05,000001,Vente,185000,5080,,CHE DE VOGELAS,0471,01370,01426,Val-Revermont,01,,,01426312ZC0122,,,,,,,,,,,,,0,3,DÃ©pendance,,0,S,sols,,,2410,5.386094,46.32714\n",
        "2021-2,2021-01-06,000001,Vente,10,,,ROUGEMONT,B043,01290,01042,Bey,01,,,010420000A0204,,,,,,,,,,,,,0,,,,,BT,taillis simples,,,530,4.844368,46.224233\n",
        "2021-3,2021-01-04,000001,Vente,204332,7,,ALL DES ECUREUILS,0276,01310,01065,Buellas,01,,,010650000B1325,,,,,,,,,,,,,0,1,Maison,88,4,S,sols,,,866,5.157688,46.200988\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSFQkzNlj-l6"
      },
      "source": [
        "### Charger le fichier csv dans un objet DataFrame Pandas\n",
        "\n",
        "Ce fichier Colab, comme beaucoup d'autres programmes de Machine Learning, rassemble les fichiers csv et les stocke en mémoire en tant qu'objet DataFrame Pandas.\n",
        "Pandas est une bibliothèque open source Python.\n",
        "Le type de structure de données primaire dans Pandas est le DataFrame.\n",
        "Voyez le DataFrame comme une feuille de calcul dans laquelle chaque ligne est identifiée par une ligne et chaque colonne par un nom. Pandas s'appuie lui-même sur une bibliothèque Python open source appelé NumPy. Si vous n'êtes pas familier avec toutes ces technologies, vous pouvez consulter les liens suivants ou bien refaire les exercices précédents du cours !\n",
        "\n",
        "*   [NumPy](https://colab.research.google.com/github/google/eng-edu/blob/main/ml/cc/exercises/numpy_ultraquick_tutorial.ipynb?utm_source=linearregressionreal-colab&utm_medium=colab&utm_campaign=colab-external&utm_content=numpy_tf2-colab&hl=en)\n",
        "*   [Pandas DataFrames](https://colab.research.google.com/github/google/eng-edu/blob/main/ml/cc/exercises/pandas_dataframe_ultraquick_tutorial.ipynb?utm_source=linearregressionreal-colab&utm_medium=colab&utm_campaign=colab-external&utm_content=pandas_tf2-colab&hl=en)\n",
        "\n",
        "Le code suivant importe le fichier csv dans un DataFrame Pandas et remet à l'échelle les valeurs contenues dans le label `median_house_value`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZlvdpyYKx7V"
      },
      "source": [
        "# Import the dataset.\n",
        "training_df = pd.read_csv(filepath_or_buffer=\"https://download.mlcc.google.com/mledu-datasets/california_housing_train.csv\")\n",
        "\n",
        "# Scale the label.\n",
        "training_df[\"median_house_value\"] /= 1000.0\n",
        "\n",
        "# Print the first rows of the pandas DataFrame.\n",
        "training_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5inxx49n4U9u"
      },
      "source": [
        "Mettre à l'échelle `median_house_value` permet de changer l'unité de la valeur de chaque au millier. Le fait de mettre à l'échelle va nous permettre de conserver des valeurs d'écart (loss) et de taux d'apprentissage (learning rates) dans des intervalles plus pratiques.\n",
        "\n",
        "Bien que mettre à l'échelle le label n'est *pas* essentielle, la mise à l'échelle des caractéristiques (features) dans un modèle à multi-caractéristique *est* essentielle.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMysi6-3IAbu"
      },
      "source": [
        "## Analyser le dataset\n",
        "\n",
        "Une grande partie des projets de machine learning consiste à bien connaître vos données.\n",
        "L'API Pandas fournit une fonction `describe` qui affiche les statistiques suivantes pour chaque colonne dans l'objet DataFrame :\n",
        "\n",
        "* `count`, qui est le nombre de lignes dans la colonne. Normalement, `count` renvoie la même valeur pour chaque colonne. \n",
        "\n",
        "* `mean` et `std`, qui correspondent à la moyenne et l'écart type des valeurs de chaque colonne. \n",
        "\n",
        "* `min` et `max`, qui correspondent à la plus petite et plus grande valeur de chaque colonne.\n",
        "\n",
        "* `25%`, `50%`, `75%`, qui correspondent aux différents [quartiles](https://developers.google.com/machine-learning/glossary/#quantile)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnUSYKw4LUuh"
      },
      "source": [
        "# Get statistics on the dataset.\n",
        "training_df.describe()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9pcW_Yjtoo8"
      },
      "source": [
        "### Exercice 1: Identifier les anomalies dans le jeu de données\n",
        "\n",
        "Voyez-vous des anomalies (valeurs étranges) dans les données ?\n",
        "Pensez à observer la répartition des données et à la définition des quartiles."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3014ezH3C7jT"
      },
      "source": [
        "## Définir les fonctions de création et d'entraînement de modèle\n",
        "\n",
        "Le code suivant définit 2 fonctions :\n",
        "\n",
        "  * `build_model(my_learning_rate)`, qui construit un modèle initialisé avec des valeurs aléatoires.\n",
        "  * `train_model(model, feature, label, epochs)`, qui entraîne le modèle à partir des données exemples (caractéristiques et labels) que vous lui passez. \n",
        "\n",
        "Pas la peine de modifier le code.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pedD5GhlDC-y"
      },
      "source": [
        "#@title Define the functions that build and train a model\n",
        "def build_model(my_learning_rate):\n",
        "  \"\"\"Create and compile a simple linear regression model.\"\"\"\n",
        "  # Most simple tf.keras models are sequential.\n",
        "  model = tf.keras.models.Sequential()\n",
        "\n",
        "  # Describe the topography of the model.\n",
        "  # The topography of a simple linear regression model\n",
        "  # is a single node in a single layer.\n",
        "  model.add(tf.keras.layers.Dense(units=1, \n",
        "                                  input_shape=(1,)))\n",
        "\n",
        "  # Compile the model topography into code that TensorFlow can efficiently\n",
        "  # execute. Configure training to minimize the model's mean squared error. \n",
        "  model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=my_learning_rate),\n",
        "                loss=\"mean_squared_error\",\n",
        "                metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
        "\n",
        "  return model        \n",
        "\n",
        "\n",
        "def train_model(model, df, feature, label, epochs, batch_size):\n",
        "  \"\"\"Train the model by feeding it data.\"\"\"\n",
        "\n",
        "  # Feed the model the feature and the label.\n",
        "  # The model will train for the specified number of epochs. \n",
        "  history = model.fit(x=df[feature],\n",
        "                      y=df[label],\n",
        "                      batch_size=batch_size,\n",
        "                      epochs=epochs)\n",
        "\n",
        "  # Gather the trained model's weight and bias.\n",
        "  trained_weight = model.get_weights()[0]\n",
        "  trained_bias = model.get_weights()[1]\n",
        "\n",
        "  # The list of epochs is stored separately from the rest of history.\n",
        "  epochs = history.epoch\n",
        "  \n",
        "  # Isolate the error for each epoch.\n",
        "  hist = pd.DataFrame(history.history)\n",
        "\n",
        "  # To track the progression of training, we're going to take a snapshot\n",
        "  # of the model's root mean squared error at each epoch. \n",
        "  rmse = hist[\"root_mean_squared_error\"]\n",
        "\n",
        "  return trained_weight, trained_bias, epochs, rmse\n",
        "\n",
        "print(\"Defined the create_model and traing_model functions.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ak_TMAzGOIFq"
      },
      "source": [
        "## Définir les fonctions de visualisation\n",
        "\n",
        "La fonction [matplotlib](https://developers.google.com/machine-learning/glossary/#matplotlib) ci-après crée les graphiques suivants :\n",
        "\n",
        "*  a graphique de points qui représente la caractéristique en fonction du label, et une ligne qui montre la sortie du modèle entraîné\n",
        "*  la courbe de l'écart calculé\n",
        "\n",
        "Pas la peine de modifier le code ci-après.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QF0BFRXTOeR3"
      },
      "source": [
        "#@title Define the plotting functions\n",
        "def plot_the_model(trained_weight, trained_bias, feature, label):\n",
        "  \"\"\"Plot the trained model against 200 random training examples.\"\"\"\n",
        "\n",
        "  # Label the axes.\n",
        "  plt.xlabel(feature)\n",
        "  plt.ylabel(label)\n",
        "\n",
        "  # Create a scatter plot from 200 random points of the dataset.\n",
        "  random_examples = training_df.sample(n=200)\n",
        "  plt.scatter(random_examples[feature], random_examples[label])\n",
        "\n",
        "  # Create a red line representing the model. The red line starts\n",
        "  # at coordinates (x0, y0) and ends at coordinates (x1, y1).\n",
        "  x0 = 0\n",
        "  y0 = trained_bias\n",
        "  x1 = 10000\n",
        "  y1 = trained_bias + (trained_weight * x1)\n",
        "  plt.plot([x0, x1], [y0, y1], c='r')\n",
        "\n",
        "  # Render the scatter plot and the red line.\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def plot_the_loss_curve(epochs, rmse):\n",
        "  \"\"\"Plot a curve of loss vs. epoch.\"\"\"\n",
        "\n",
        "  plt.figure()\n",
        "  plt.xlabel(\"Epoch\")\n",
        "  plt.ylabel(\"Root Mean Squared Error\")\n",
        "\n",
        "  plt.plot(epochs, rmse, label=\"Loss\")\n",
        "  plt.legend()\n",
        "  plt.ylim([rmse.min()*0.97, rmse.max()])\n",
        "  plt.show()  \n",
        "\n",
        "print(\"Defined the plot_the_model and plot_the_loss_curve functions.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-IXYVfvM4gD"
      },
      "source": [
        "## Appeler les fonctions du modèle\n",
        "\n",
        "Une part importante du machine learning est de déterminer quelles [features (caractéristiques)](https://developers.google.com/machine-learning/glossary/#feature) sont corrélées au [label](https://developers.google.com/machine-learning/glossary/#label). Par exemple, dans la vie réelle, les modèles de prédiction de valeurs foncières s'appuient sur des centaines de caractéristiques. En revanche, notre modèle va s'appuyer sur une unique caractéristique. Pour l'instant, vous prenez de manière arbitraire la caractéristique `total_rooms`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nj3v5EKQFY8s",
        "cellView": "both"
      },
      "source": [
        "# The following variables are the hyperparameters.\n",
        "learning_rate = 0.01\n",
        "epochs = 30\n",
        "batch_size = 30\n",
        "\n",
        "# Specify the feature and the label.\n",
        "my_feature = \"total_rooms\"  # the total number of rooms on a specific city block.\n",
        "my_label=\"median_house_value\" # the median value of a house on a specific city block.\n",
        "# That is, you're going to create a model that predicts house value based \n",
        "# solely on total_rooms.  \n",
        "\n",
        "# Discard any pre-existing version of the model.\n",
        "my_model = None\n",
        "\n",
        "# Invoke the functions.\n",
        "my_model = build_model(learning_rate)\n",
        "weight, bias, epochs, rmse = train_model(my_model, training_df, \n",
        "                                         my_feature, my_label,\n",
        "                                         epochs, batch_size)\n",
        "\n",
        "print(\"\\nThe learned weight for your model is %.4f\" % weight)\n",
        "print(\"The learned bias for your model is %.4f\\n\" % bias )\n",
        "\n",
        "plot_the_model(weight, bias, my_feature, my_label)\n",
        "plot_the_loss_curve(epochs, rmse)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Btp8zUNbYOcd"
      },
      "source": [
        "Une grande part d'aléatoire intervient dans l'entraînement d'un modèle. Par conséquent, vous obtiendrez des résultats différents à chaque fois que vous entraînez un modèle identique.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xNqWWos_zyk"
      },
      "source": [
        "## Utiliser le modèle entraîné pour réaliser des prédictions\n",
        "\n",
        "Vous pouvez utiliser le modèle entraîné pour réaliser des prédictions. En pratique, [vous devriez faire des prédictions sur des données qui n'ont pas servi à l'entraînement](https://developers.google.com/machine-learning/crash-course/training-and-test-sets/splitting-data). Cepedant, pour cet exercice, vous allez travailler avec un sous ensemble du jeu de données d'entraînement. Plus loin, nous aborderons ce point.\n",
        "\n",
        "Pour commener, exécuter le code suivant pour créer une fonction qui va réaliser une prédiction.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nH63BmncAcab"
      },
      "source": [
        "def predict_house_values(n, feature, label):\n",
        "  \"\"\"Predict house values based on a feature.\"\"\"\n",
        "\n",
        "  batch = training_df[feature][10000:10000 + n]\n",
        "  predicted_values = my_model.predict_on_batch(x=batch)\n",
        "\n",
        "  print(\"feature   label          predicted\")\n",
        "  print(\"  value   value          value\")\n",
        "  print(\"          in thousand$   in thousand$\")\n",
        "  print(\"--------------------------------------\")\n",
        "  for i in range(n):\n",
        "    print (\"%5.0f %6.0f %15.0f\" % (training_df[feature][10000 + i],\n",
        "                                   training_df[label][10000 + i],\n",
        "                                   predicted_values[i][0] ))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NbBNQujU5WjK"
      },
      "source": [
        "Maintenant, appeler cette fonction sur 10 exemples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_0DGBt0Kz_N"
      },
      "source": [
        "predict_house_values(?, ?, ?)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-gGaqArcpqY3"
      },
      "source": [
        "### Exercice 2: Evaluer la qualité de la prédiction du modèle\n",
        "\n",
        "Jeter un oeil à la table précédente. Est-ce que les données prédites sont proches des données attendues ?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLoqis3IUPSd"
      },
      "source": [
        "## Exercice 3: Essayer une autre caractéristique\n",
        "\n",
        "La caractéristique `total_rooms` avait un faible pouvoir de prédiction. Essayer maintenant avec la caractéristique `population`.\n",
        "\n",
        "Note: Si vous changez les caractéristiques, il est fort probable que vous changerez les hyperparamètres."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0ab6HD4ZO75"
      },
      "source": [
        "my_feature = ?   # Replace the ? with population or possibly\n",
        "                   # a different column name.\n",
        "\n",
        "# Experiment with the hyperparameters.\n",
        "learning_rate = ?\n",
        "epochs = ?\n",
        "batch_size = ?\n",
        "\n",
        "# Don't change anything below this line.\n",
        "my_model = build_model(learning_rate)\n",
        "weight, bias, epochs, rmse = train_model(my_model, training_df, \n",
        "                                         my_feature, my_label,\n",
        "                                         epochs, batch_size)\n",
        "plot_the_model(weight, bias, my_feature, my_label)\n",
        "plot_the_loss_curve(epochs, rmse)\n",
        "\n",
        "predict_house_values(15, my_feature, my_label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nd_rHJ59AUtk"
      },
      "source": [
        "Est-ce que la colonne `population` produit de meilleurs résultats que `total_rooms`?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8uYpyGacsIg"
      },
      "source": [
        "## Exercice 4: Créer une caractéristique supplémentaire\n",
        "\n",
        "Vous avez constaté que les caractéristiques `total_rooms` et `population` n'était pas pertinentes. Cela ne sera proablement pas le cas non plus pour les autres colonnes.\n",
        "Peut être que le *ratio* de `total_rooms` par `population` pourrait avoir un pouvoir de prédiction plus important. Et peut être que la densité à un rapport avec la valeur immobilière.\n",
        "\n",
        "Pour tester ces hypothèses, réaliser les actions suivantes :\n",
        "\n",
        "1. Créer une [caractéristique supplémentaire](https://developers.google.com/machine-learning/glossary/#synthetic_feature) qui est le ratio entre `total_rooms` et `population`. (Si vous êtes nouveau avec les dataframes Pandas, merci de suivre le tutorial suivant [Pandas DataFrame Ultraquick Tutorial](https://colab.research.google.com/github/google/eng-edu/blob/main/ml/cc/exercises/pandas_dataframe_ultraquick_tutorial.ipynb?utm_source=linearregressionreal-colab&utm_medium=colab&utm_campaign=colab-external&utm_content=pandas_tf2-colab&hl=en).)\n",
        "2. Affiner les 3 hyperparamètres\n",
        "3. Déterminer si la nouvelle caractéristiques supplémentaire permet de réduire l'écart par rapport à la simple caractéristique que vous avez essayé auparavant."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Kx2xHSgdcpg"
      },
      "source": [
        "# Define a synthetic feature named rooms_per_person\n",
        "training_df[\"rooms_per_person\"] = ? # write your code here.\n",
        "\n",
        "# Don't change the next line.\n",
        "my_feature = \"rooms_per_person\"\n",
        "\n",
        "# Assign values to these three hyperparameters.\n",
        "learning_rate = ?\n",
        "epochs = ?\n",
        "batch_size = ?\n",
        "\n",
        "# Don't change anything below this line.\n",
        "my_model = build_model(learning_rate)\n",
        "weight, bias, epochs, rmse = train_model(my_model, training_df,\n",
        "                                         my_feature, my_label,\n",
        "                                         epochs, batch_size)\n",
        "\n",
        "plot_the_loss_curve(epochs, rmse)\n",
        "predict_house_values(15, my_feature, my_label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HBiDWursB1Wi"
      },
      "source": [
        "En regardant la coubre de l'écart, cette caractéristique supplémentaire produit un bien meilleur modèle. Cependant, le modèle fait toujours des prédictions à côté de la plaque.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XEG_9oU9O54u"
      },
      "source": [
        "## Exercice 5. Trouver la caractéristique dont les valeurs brutes sont les mieux corrélées avec le label résultat\n",
        "\n",
        "Maintenant que nous nous somes appuyés sur le tatonnement pour identifier les potentielles caractéristiques liées au label, essayons de travailler les statistiques.\n",
        "\n",
        "Une **matrice de corrélation** indique comment chaque valeur brute d'attribute est lié aux autres valeurs brutes d'attribut. Les valeurs de corrélation ont la signification suivante :\n",
        "\n",
        "  * `1.0`: corrélation positive parfaite. Quand cet attribut augmente, l'autre attribut augmente également..\n",
        "  * `-1.0`: corrélation négative parfait, quand un attribut augmente, l'autre diminue. \n",
        "  * `0.0`: pas de corrélation; les 2 colonnes [ne sont pas corrélées](https://en.wikipedia.org/wiki/Correlation_and_dependence#/media/File:Correlation_examples2.svg).\n",
        "\n",
        "En général, plus la valeur absolue de la valeur de corréaltion est importante, plus le pouvoir de prédiction est grand. Par exemple, une valeur de corrélation de -0.8 implique un pouvoir de prédiction plus important qu'une valeur de corréaltion de 0.2\n",
        "Ci-après le code pour générer la matrice de corrélation :\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zFGKL45LO8Tt"
      },
      "source": [
        "# Generate a correlation matrix.\n",
        "training_df.corr()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hp0r3NAVPEdt"
      },
      "source": [
        "Quelle est la caractéristique la plus appropriée ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8RqvEbaVSlRt"
      },
      "source": [
        "Les matrices de corrélation ne sont pas l'unique outil mais sont un excellent moyen pour démarrer.\n"
      ]
    }
  ]
}